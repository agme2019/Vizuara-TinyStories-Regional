<div align="center">

  <h1> TinyStories Regional <img src="https://png.pngtree.com/png-vector/20220812/ourmid/pngtree-indian-flag-design-png-png-image_6108311.png" width="30"> </h1>
  <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a1a36ff-6d9a-4ee7-9494-3ae38adfe134_1920x600.png" alt="Vizuara Logo" style="width:80%;">

  [![arXiv](https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg?style=flat)](https://arxiv.org/abs/1234.56789)
  [![Huggingfaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/TinyStories-Regional)

  [![nirvan](https://img.shields.io/badge/nirvan-black?logo=github&logoColor=white&labelColor=black&color=black&style=flat)](https://github.com/nirvan840)
  [![malhar](https://img.shields.io/badge/malhar-black?logo=github&logoColor=white&labelColor=black&color=black&style=flat)](https://github.com/malharinamdar)
  [![agnivo](https://img.shields.io/badge/agnivo-black?logo=github&logoColor=white&labelColor=black&color=black&style=flat)](https://github.com/agme2019)
  [![raj](https://img.shields.io/badge/üåê-rajdandekar-black?logo=globe&logoColor=white&labelColor=black&color=black&style=flat)](https://www.linkedin.com/in/raj-abhijit-dandekar-67a33118a/?originalSubdomain=in)
  
</div>
<br>

> [!IMPORTANT]
> * <i> Currently, models only support story completion based on starting prompts :) </i>
> * <i> Guides to using the repository, datasets and the models are below! </i>

> [!NOTE]
> #### _A special thanks to_
> * <i> <a href="https://tensordock.com/">TensorDock</a> for providing GPU compute! Check them out for easy-to-deploy and cheap GPU/CPU VMs üíö </i>
> * <i> Microsoft for inspiring us with their original <a href="https://arxiv.org/abs/2305.07759">TinyStories</a> paper üíô </i>
> * <i> <a href="https://huggingface.co/sarvamai">Sarvam</a>, <a href="https://huggingface.co/TWO">SUTRA</a>, and <a href="https://karpathy.ai/">Andrej Karpathy</a> for their open-source efforts ‚ù§Ô∏è </i>

> [!WARNING]
> * Our TinyStories Regional paper will soon be on arXiv!
> * Any references to the paper below are currently not accessible 

```sh
git clone https://github.com/nirvan840/Vizuara-TinyStories-Regional.git
```
```sh
pip install g4f[all] aiolimter transformers datasets huggingface_hub sentencepiece tiktoken wandb tqdm torch numpy 
```

---

<br> 

# üìö Table of Contents

- ### üóÇÔ∏è Dataset Generation
  - #### [‚úçÔ∏è Preparing Prompts](#preparing-prompts)
  - #### [üí¨ Prompting a Model](#prompting-a-model)
- ### ‚öôÔ∏è Training SLMs
  - #### [üî§ Tokenizing Data](#tokenizing-data)
  - #### [üèãÔ∏è Training the Model](#training-the-model)
- ### üîç Inference and Evaluation
  - #### [ü§ñ SLM Inference](#inference-models-local-or-hf)
  - #### [üìä Evaluate Inference/Stories](#evaluate-inference-stories)
- ### üìà Results
  - #### [üí° Inference Examples](#inference-examples)
  - #### [‚úÖ A Fitting Use Case](#a-fitting-use-case)
- ### üí∞ Costs
  - #### [‚è±Ô∏è Training Time and Costs](#training-time-and-costs)
  - #### [üîÑ Replicating the Project](#replicating-the-project)

---

<br> 

# üóÇÔ∏è Dataset Generation 

> [!WARNING] 
> <i> This repository provides code to generate data by making API calls to SOTA models (4o, 4o-mini, Gemini-flash-1.5, etc.) using the [GPT-4-free (G4F)](https://github.com/xtekky/gpt4free) repository. We do not condone using this repository for large-scale dataset generation; we include it as a free alternative to paid API services. Please read and follow official OpenAI/Gemini guidelines. </i>

> [!NOTE]
> - Our datasets for Hindi, Marathi and Bangla, generated using GPT-4o-mini, are open-sourced on our HF
> - Translated versions (Hindi and Bangla) of [Microsoft's TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset can also be found on our HF
> - Translated versions (our Hindi ‚û°Ô∏è Bangla ; Hindi ‚û°Ô∏è Marathi ; Bangla ‚û°Ô∏è Hindi) will be soon on our HF

<h2 id="preparing-prompts">‚úçÔ∏è Preparing Prompts</h2>
<ul>
  <p><li>Each prompt is generated by sampling a unique set of a noun, a verb, an adjective and a feature</li></p>
  <p><li>To modify the list of nouns, verbs, etc., please modify <code>.txt</code> files at <code>prompting/prompt_gen/<i>&lt;lanauge&gt;</i></code></li></p>
  <p></p><li>Prompt templates/complexities can be referred to/modified through <code>prompting/prompt_gen/create_prompts.py</code>.</li> 
      <ul> <li>We compare various complexities in our paper and find <code>2+</code> to be optimal</li> </ul></p>
  <p><li>Unique (sampling is unique and not random) prompts can be generated by running <code>generate_prompts.py</code></li></p>
  <p><li>Generated prompts are written to a <code>.json</code> file. Sharding the file is recommended (below)</li></p>
</ul>

<i> To generate prompts please run: </i>
```sh
python prompting/prompt_gen/generate_prompts.py
```

<h2 id="prompting-a-model">üí¨ Prompting a Model</h2>
<ul>
  <p><li>Generated prompts are read from the file/shards and "sent" to the specified LLM (using <a href="https://github.com/xtekky/gpt4free">G4F</a>)</li></p> 
  <p><li>Multithreaded API calls result in a max speed of <code>100 stories/min</code> for GPT-4o-mini and GPT-4o (occasionally).</li>
      <ul>
        <li>It is recommended number of threads = number of <code>4 x vCPUs</code></li>
        <li>Each thread writes to a common file. Once every 10% of total progress</li>
        <li><i>Optimal config</i>: <code>16 vCPUs</code>, running <code>4 sessions</code> concurrently (one for each shard), each with <code>16 threads</code></li>
      </ul></p>
  <p><li>Please look into <code>prompting/make_requests.py</code> for customizing the prompting schema</li></p>
  <p><li>Please look into <code>prompting/request_helper.py</code> for a detailed look into the process</li>
      <ul>
        <li>API/LLM is prompted until a valid story is generated for each prompt</li>
        <li>Various regex (data cleanup) features</li>
      </ul></p>
</ul>

<i>To start the data generation process, please run the script: </i>
```sh
python prompting/make_requests.py
```
_TIPüí°: To run data generation in the background (detached VM session)_
```sh
tmux new -s session_name
```

---

<br>

# ‚öôÔ∏è Training Small Language Models (SLMs) 

> [!NOTE]
> * We utilize Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT) repository (with modifications) to train models
> * Our training script supports multi-GPU training (DDP), progress (TQDM) and logging support (WANDB), along with easy customizability

> [!IMPORTANT]
> * It is essential that data is tokenized correctly <bos token> story <eos token> (read below)
> * Lower end GPUs <code>T4</code> (Google Collab), <code>P100</code> (Kaggle) can be used to train models in <code><24hrs</code> on our datasets!


<h1 id="tokenizing-data">üî§ Tokenizing Data</h1>
<ul>
  <p><li>The entire dataset is tokenized before training. Token IDs are stored in <code>.bin</code> files
      <ul>
        <li>The dataset to be tokenized can be chosen/specified as per <code>training-inference/data/prepare.py line 38-46</code></li>
        <li>The <code>.bin</code> files must be appropriately placed in a folder (specified in config.py) in <code>training-inference/data/</code></li>
        <li>Use <code>training-inference/data/decode_data.py</code> to decode and print first 500 tokens from a <code>.bin</code> file</li>
        <li>Ensure that the decoded tokens follow the format: <code>&lt;bos token&gt; story1 &lt;eos token&gt; &lt;bos token&gt; story2 &lt;eos token&gt;...</code></li>
      </ul>
  </li></p>
  <p><li>Tokenization is carried out by the script <code>training-inference/data/prepare.py</code></li></p>
  <p><li>We provide direct support for the following tokenizers:
      <ul>
        <li><a href="https://huggingface.co/sarvamai/sarvam-1">Sarvam (sarvam-1)</a> (Indic)</li>
        <li><a href="https://huggingface.co/TWO/sutra-mlt256-v2">Sutra (mlt256-v2)</a> (Indic)</li>
        <li><a href="https://github.com/openai/tiktoken">Tiktoken (GPT-2)</a> (English)</li>
      </ul>
  </li></p>
  <p><li>We provide easy support for any additional tokenizers available on HF.
      <ul>
        <li>Specify the new tokenizer along similar lines as<code>training-inference/data/prepare.py line 19-35</code></li>
        <li>For this to work, HF tokenizers must have valid <code>eos</code> and <code>bos</code> tokens</li>
      </ul>
  </li></p>
</ul>

_To tokenize an HF dataset, run the script:_
```sh
python training-inference/data/prepare.py
```

<h2 id="training-the-model">üèãÔ∏è Training the Model</h2>
<ul>
  <p><li>Training can be resumed for locally saved models as well as those from Vizuara-HF!</li></p>
  <p><li>Changes to the training configuration can be easily made through <code>training-inference/config.py</code></li></p>
  <p><li>Model weights are checkpointed every <code>eval_iters</code> and saved at <code>training-inference/out/</code> as <code>.pt</code> files</li></p>
</ul>

_To start training, run:_
```sh
# Single GPU
python training-inference/train.py training-inference/config.py
```
```sh
# Multi GPU (DDP)
torchrun --standalone --nproc_per_node=num_gpus training-inference/train.py training-inference/config.py
```
        
_Automated multi-config training:_
```sh
chmod +x training-inference/utils/automate-training.sh
./training-inference/utils/automate-training.sh
```

---

<br>

# üîç Inference and Evaluation

> [!NOTE]
> * Given the small size of the models, CPU inference is supported!
> * It is crucial to ensure tokenization occurs correctly (refer below)!
> * Prompting scripts are repurposed for evaluation of stories produced by SLMs

<h2 id="inference-models-local-or-hf">ü§ñ SLM Inference (Local or HF)</h2>
<ul>
  <p><li>Refer to the sample settings at the bottom of <code>training-inference/config.py</code></li>
      <ul>
        <li>Choose between locally saved models or those from our HF by toggling <code>load_from_hf</code></li>
        <li>Stories generated per prompt, temperature and top_k control is available</li>
      </ul>
  <p>
  <p><li>Various tokenizers can be used inference</li>
      <ul>
        <li>Explicit sup</li>
      </ul>
  <p>
  <p><li>Multiple prompts (each on a new line) can be mentioned in a text file.
      <ul>
        <li>Refer to <code>training-inference/&lt;language&gt;-prompts.txt</code></li>
        <li>Model can be asked to generate multiple unique stories for each prompt.</li>
        <li>Text and JSON outputs are supported.</li>
      </ul>
  </li></p>
</ul>

_To run inference, run the script:_
```sh
python training-inference/sample.py 
```

---

<br>

# üí∞ Costs

### _Soon!_
