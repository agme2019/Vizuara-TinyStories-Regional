<div align="center">

  <h1> TinyStories Regional <img src="https://png.pngtree.com/png-vector/20220812/ourmid/pngtree-indian-flag-design-png-png-image_6108311.png" width="30"> </h1>
  <img src="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3a1a36ff-6d9a-4ee7-9494-3ae38adfe134_1920x600.png" alt="Vizuara Logo" style="width:80%;">

  [![arXiv](https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg?style=flat)](https://arxiv.org/abs/1234.56789)
  [![Huggingfaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/TinyStories-Regional)

  [![nirvan](https://img.shields.io/badge/nirvan-black?logo=github&logoColor=white&labelColor=black&color=black&style=flat)](https://github.com/nirvan840)
  [![malhar](https://img.shields.io/badge/malhar-black?logo=github&logoColor=white&labelColor=black&color=black&style=flat)](https://github.com/malharinamdar)
  [![agnivo](https://img.shields.io/badge/agnivo-black?logo=github&logoColor=white&labelColor=black&color=black&style=flat)](https://github.com/agme2019)
  [![raj](https://img.shields.io/badge/ğŸŒ-rajdandekar-black?logo=globe&logoColor=white&labelColor=black&color=black&style=flat)](https://www.linkedin.com/in/raj-abhijit-dandekar-67a33118a/?originalSubdomain=in)
  
</div>
<br>

> [!IMPORTANT]
> * <i> Currently, models only support story completion based on starting prompts :) </i>
> * <i> Guides to replicating and using the repository and the models are below! </i>
> * <i> Quality of Life updates to various scripts soon to come! </i>

> [!NOTE]
> * <i> <a href="https://tensordock.com/">TensorDock</a> for providing GPU access! Please check them out for easy-to-deploy and cheap GPU options ğŸ’š </i>
> * <i> Special thanks to Microsoft for inspiring us with their original <a href="https://arxiv.org/abs/2305.07759">TinyStories</a> paper ğŸ’™ </i>
> * <i> <a href="https://huggingface.co/sarvamai">Sarvam</a>, <a href="https://huggingface.co/TWO">SUTRA</a>, and <a href="https://karpathy.ai/">Andrej Karpathy</a> for their open-source efforts â¤ï¸ </i>

> [!WARNING]
> * Our TinyStories Regional paper will soon be on arXiv!
> * Any references to the paper below are currently not accessible 

```sh
git clone https://github.com/nirvan840/Vizuara-TinyStories-Regional.git
```
```sh
pip install g4f[all] aiolimter transformers datasets huggingface_hub sentencepiece tiktoken wandb tqdm torch numpy 
```

---

<br> 

## ğŸ“š Table of Contents

- ğŸ—‚ï¸ Dataset Generation
  - [âœï¸ Preparing Prompts](#preparing-prompts)
  - [ğŸ’¬ Prompting a Model](#prompting-a-model)
- âš™ï¸ Training SLMs
  - [ğŸ”¤ Tokenizing Data](#tokenizing-data)
  - [ğŸ‹ï¸ Training the Model](#training-the-model)
- ğŸ” Inference and Evaluation
  - [ğŸ¤– Inference Models (Local or HF)](#inference-models-local-or-hf)
  - [ğŸ“Š Evaluate Inference/Stories](#evaluate-inference-stories)
- ğŸ“ˆ Results
  - [ğŸ’¡ Inference Examples](#inference-examples)
  - [âœ… A Fitting Use Case](#a-fitting-use-case)
- ğŸ’° Costs
  - [â±ï¸ Training Time and Costs](#training-time-and-costs)
  - [ğŸ”„ Replicating the Project](#replicating-the-project)

---

<br> 

## ğŸ—‚ï¸ Dataset Generation 

> [!WARNING] 
> <i> This repository provides code to generate data by making API calls to SOTA models (4o, 4o-mini, Gemini-flash-1.5, etc.) using the [GPT-4-free (G4F)](https://github.com/xtekky/gpt4free) repository. We do not condone using this repository for large-scale dataset generation; we include it as a free alternative to paid API services. Please read and follow official OpenAI/Gemini guidelines. </i>

> [!NOTE]
> - Our datasets for Hindi, Marathi and Bangla, generated using GPT-4o-mini, are open-sourced on our HF
> - Translated versions (Hindi and Bangla) of [Microsoft's TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset can also be found on our HF
> - Translated versions (our Hindi â¡ï¸ Bangla ; Hindi â¡ï¸ Marathi ; Bangla â¡ï¸ Hindi) will be soon on our HF

<h2 id="preparing-prompts">âœï¸ Preparing Prompts</h2>
<ul>
  <p><li>Prompts are generated by sampling a unique set of a noun, a verb, an adjective and a feature</li></p>
  <p><li>To modify the list of nouns, verbs, etc., please modify <code>.txt</code> files at <code>prompting/prompt_gen/<i>lanauge</i></code></li></p>
  <p></p><li>Prompt templates/complexities can be referred to/modified through <code>prompting/prompt_gen/create_prompts.py</code>.</li> 
      <ul> <li>We compare various complexities in our paper and find <code>2+</code> to be optimal</li> </ul></p>
  <p><li>Unique (sampling is unique and not random) prompts can be generated by running <code>generate_prompts.py</code></li></p>
  <p><li>Generated prompts are written to a <code>.json</code> file. Sharding the file is recommended (below)</li></p>
</ul>

<i> To generate prompts please run: </i>
```sh
python prompting/prompt_gen/generate_prompts.py
```

<h2 id="prompting-a-model">ğŸ’¬ Prompting a Model</h2>
<ul>
  <p><li>Generated prompts are read from the file/shards and "sent" to the specified LLM (using <a href="https://github.com/xtekky/gpt4free">G4F</a>)</li></p> 
  <p><li>Multithreaded API calls result in a max speed of <code>100 stories/min</code> for GPT-4o-mini and GPT-4o (occasionally).</li>
      <ul>
        <li>It is recommended number of threads = number of <code>4 x vCPUs</code></li>
        <li>Each thread writes to a common file. Once every 10% of total progress</li>
        <li><i>Optimal config</i>: <code>16 vCPUs</code>, running <code>4 sessions</code> concurrently (one for each shard), each with <code>16 threads</code></li>
      </ul></p>
  <p><li>Please look into <code>prompting/make_requests.py</code> for customizing the prompting schema</li></p>
  <p><li>Please look into <code>prompting/request_helper.py</code> for a detailed look into the process</li>
      <ul>
        <li>Various regex (data cleanup) features</li>
        <li>API/LLM is prompted until a valid story is generated for each prompt</li>
      </ul></p>
</ul>

<i>To start the data generation process, please run the script: </i>
```sh
python prompting/make_requests.py
```
_TIPğŸ’¡: To run data generation in the background (detached VM session)_
```sh
tmux new -s session_name
```

---

<br>

## âš™ï¸ Training Small Language Models (SLMs) 

> [!NOTE]
> * We utilize Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT) repository (with modifications) to train models
> * Our training script supports multi-GPU training (DDP), progress (TQDM) and logging support (WANDB), along with easy customizability

> [!IMPORTANT]
> * PyTroch <code>torch</code> required for training!
> * Training our SLMs is compute-friendly!
> * Lower end GPUs <code>T4</code> (Google Collab), <code>P100</code> (Kaggle) can be used to train models in <code><24hrs</code> on our datasets!

<h2 id="#tokenizing-data">ğŸ”¤ Tokenizing Data</h2>
<ul>
  <p><li>The entire dataset is tokenized before training. Token IDs are stored in <code>.bin</code> files</li></p>
  <p><li>Tokenization is carried out by the script <code>training-inference/data/prepare.py</code></li></p>
  <p><li>We provide direct support for the following tokenizers:
      <ul>
        <li><a href="https://huggingface.co/sarvamai/sarvam-1">Sarvam</a> (Indic)</li>
        <li><a href="https://huggingface.co/TWO/sutra-mlt256-v2/tree/main">Sutra</a> (Indic)</li>
        <li><a href="https://github.com/openai/tiktoken">Tiktoken</a> (GPT-2)</li>
      </ul>
  </li></p>
  <p><li>We provide easy support for any additional tokenizers on HF.
      <ul>
        <li>Only include <code>hf</code> in the tokenizer name and add its HF repository to <code>prepare.py</code>!</li>
        <li>For this to work, HF tokenizers must have valid <code>eos</code> and <code>bos</code> tokens</li>
      </ul>
  </li></p>
</ul>

_To tokenize an HF dataset, choose/specify it in <code>trainin-inference/data/prepare.py</code> and run the script:_
```sh
python training-inference/data/prepare.py
```

<h2 id="#training-the-model">ğŸ‹ï¸ Training the Model</h2>
<ul>
  <p><li>Training can be resumed for locally saved models as well as those from Vizuara-HF!</li></p>
  <p><li>Changes to the training configuration can be easily made through <code>training-inference/config.py</code></li></p>
  <p><li>Model weights are checkpointed every <code>eval_iters</code> and saved at <code>training-inference/out/</code> as <code>.pt</code> files</li></p>
</ul>

_To start training, run:_
```sh
# Single GPU
python training-inference/train.py training-inference/config.py
```
```sh
# Multi GPU (DDP)
torchrun --standalone --nproc_per_node=num_gpus training-inference/train.py training-inference/config.py
```
        
_Automated multi-config training:_
```sh
chmod +x automate-training.sh
./automate-training.sh
```

---

<br>

## ğŸ” Inference and Evaluation
